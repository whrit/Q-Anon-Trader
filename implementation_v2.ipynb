{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1cf939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "from src.environment import StockTradingEnvironment\n",
    "from src.utils import save_pickle, load_pickle, plot_grid\n",
    "from src.learner import q_learning_learning_loop\n",
    "import yfinance as yf\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "# Logging setup for better tracking of progress\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    logging.info(\"Using GPU for training\")\n",
    "else:\n",
    "    logging.info(\"Using CPU for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169c51c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yfinance download data\n",
    "def fetch_stock_data(symbol, start_date, end_date, output_file):\n",
    "    stock_data = yf.download(symbol, start=start_date, end=end_date)\n",
    "    stock_data['Close'] = stock_data['Adj Close']\n",
    "    \n",
    "    stock_data = stock_data.drop(columns=['Adj Close'])\n",
    "\n",
    "    stock_data.to_csv(output_file)\n",
    "        \n",
    "    return stock_data\n",
    "\n",
    "stock_data = fetch_stock_data('AAPL', '2022-01-01', '2024-06-19', 'AAPL_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bd4c31",
   "metadata": {},
   "source": [
    "#### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5f3ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleQLearningAgent:\n",
    "    def __init__(self, env, learning_rate, discount_factor):\n",
    "        self.env = env\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        # Use a dictionary to handle the Q-tables with discrete state-action pairs\n",
    "        self.q_table1 = {}\n",
    "        self.q_table2 = {}\n",
    "\n",
    "    def step(self, state, epsilon):\n",
    "        state_index = self._get_state_index(state)\n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.choice(self.env.action_space.n)\n",
    "        else:\n",
    "            q_values = self.q_table1.get(state_index, np.zeros(self.env.action_space.n)) + self.q_table2.get(state_index, np.zeros(self.env.action_space.n))\n",
    "            return np.argmax(q_values)\n",
    "\n",
    "    def update_qvalue(self, state, action, reward, next_state):\n",
    "        state_index = self._get_state_index(state)\n",
    "        next_state_index = self._get_state_index(next_state)\n",
    "        if np.random.rand() < 0.5:\n",
    "            best_next_action = np.argmax(self.q_table1.get(next_state_index, np.zeros(self.env.action_space.n)))\n",
    "            self.q_table1[state_index] = self.q_table1.get(state_index, np.zeros(self.env.action_space.n))\n",
    "            self.q_table1[state_index][action] += self.learning_rate * (\n",
    "                reward + self.discount_factor * self.q_table2.get(next_state_index, np.zeros(self.env.action_space.n))[best_next_action] - self.q_table1[state_index][action]\n",
    "            )\n",
    "        else:\n",
    "            best_next_action = np.argmax(self.q_table2.get(next_state_index, np.zeros(self.env.action_space.n)))\n",
    "            self.q_table2[state_index] = self.q_table2.get(state_index, np.zeros(self.env.action_space.n))\n",
    "            self.q_table2[state_index][action] += self.learning_rate * (\n",
    "                reward + self.discount_factor * self.q_table1.get(next_state_index, np.zeros(self.env.action_space.n))[best_next_action] - self.q_table2[state_index][action]\n",
    "            )\n",
    "\n",
    "    def _get_state_index(self, state):\n",
    "        return tuple(state.flatten())\n",
    "\n",
    "# Updated DQNAgent class\n",
    "class DQNAgent:\n",
    "    def __init__(self, env, learning_rate, discount_factor, buffer_size=10000, batch_size=32):\n",
    "        self.env = env\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        model = models.Sequential()\n",
    "        input_dim = np.prod(self.env.observation_space.shape)\n",
    "        model.add(layers.Dense(24, input_dim=input_dim, activation='relu'))\n",
    "        model.add(layers.Dense(24, activation='relu'))\n",
    "        model.add(layers.Dense(self.env.action_space.n, activation='linear'))\n",
    "        model.compile(optimizer=optimizers.Adam(learning_rate=self.learning_rate), loss='mse')\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state, epsilon):\n",
    "        if np.random.rand() <= epsilon:\n",
    "            return np.random.choice(self.env.action_space.n)\n",
    "        state = state.flatten().reshape(1, -1)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            state = state.flatten().reshape(1, -1)\n",
    "            next_state = next_state.flatten().reshape(1, -1)\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.discount_factor * np.amax(self.target_model.predict(next_state)[0])\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        self.update_target_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22706b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_experience(env, agent, epsilon):\n",
    "    obs, _ = env.reset()\n",
    "    terminated, truncated = False, False\n",
    "    experiences = []\n",
    "\n",
    "    while not terminated and not truncated:\n",
    "        current_action = agent.step(obs, epsilon)\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(current_action)\n",
    "        experiences.append((obs, current_action, reward, next_obs, terminated))\n",
    "        obs = next_obs\n",
    "    \n",
    "    return experiences\n",
    "\n",
    "def adaptive_learning_rate(epoch, lr):\n",
    "    if epoch % 100 == 0 and epoch:\n",
    "        return lr * 0.1\n",
    "    return lr\n",
    "\n",
    "def decay_epsilon(episode, epsilon, min_epsilon, decay_rate):\n",
    "    return max(min_epsilon, epsilon * decay_rate)\n",
    "\n",
    "def q_learning_learning_loop(env, agent, learning_rate: float, discount_factor: float, episodes: int,\n",
    "                             min_epsilon_allowed: float, initial_epsilon_value: float,\n",
    "                             buffer_size: int = 10000, batch_size: int = 32) -> tuple:\n",
    "    epsilon = initial_epsilon_value\n",
    "    epsilon_decay_factor = np.power(min_epsilon_allowed / epsilon, 1 / episodes)\n",
    "\n",
    "    reward_across_episodes = []\n",
    "    epsilons_across_episodes = []\n",
    "\n",
    "    replay_buffer = deque(maxlen=buffer_size)\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        experiences = collect_experience(env, agent, epsilon)\n",
    "        replay_buffer.extend(experiences)\n",
    "\n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            batch = np.array(random.sample(replay_buffer, batch_size), dtype=object)\n",
    "            for b_state, b_action, b_reward, b_next_state, b_done in batch:\n",
    "                b_state = tuple(b_state.flatten())  # Convert to tuple\n",
    "                b_next_state = tuple(b_next_state.flatten())  # Convert to tuple\n",
    "                if b_done:\n",
    "                    target = b_reward\n",
    "                else:\n",
    "                    target = b_reward + discount_factor * np.max(agent.q_table1.get(b_next_state, np.zeros(env.action_space.n)) + agent.q_table2.get(b_next_state, np.zeros(env.action_space.n)))\n",
    "\n",
    "                agent.q_table1[b_state] = agent.q_table1.get(b_state, np.zeros(env.action_space.n))\n",
    "                agent.q_table1[b_state][b_action] += learning_rate * (target - agent.q_table1[b_state][b_action])\n",
    "\n",
    "        epsilon = decay_epsilon(episode, epsilon, min_epsilon_allowed, epsilon_decay_factor)\n",
    "        reward_across_episodes.append(sum([exp[2] for exp in experiences]))\n",
    "        epsilons_across_episodes.append(epsilon)\n",
    "\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            logging.info(f\"Episode {episode + 1}/{episodes} - Reward: {reward_across_episodes[-1]} - Epsilon: {epsilon}\")\n",
    "\n",
    "    logging.info(\"Trained Q-Table: %s\", agent.q_table1)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(reward_across_episodes, label='Rewards')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Cumulative Reward')\n",
    "    plt.title('Rewards over Episodes')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epsilons_across_episodes, label='Epsilon')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Epsilon')\n",
    "    plt.title('Epsilon Decay over Episodes')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return agent, reward_across_episodes, epsilons_across_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca06c6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: You can adjust the parameter 'number_of_days_to_consider'\n",
    "\n",
    "env = stock_trading_environment = StockTradingEnvironment('./AAPL_data.csv', number_of_days_to_consider=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d38c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DoubleQLearningAgent(env, learning_rate=0.01, discount_factor=0.99)\n",
    "\n",
    "agent, reward_across_episodes, epsilons_across_episodes = q_learning_learning_loop(\n",
    "    env,\n",
    "    agent,\n",
    "    learning_rate=0.01,\n",
    "    discount_factor=0.99,\n",
    "    episodes=50000,\n",
    "    min_epsilon_allowed=0.01,\n",
    "    initial_epsilon_value=1,\n",
    "    buffer_size=100000,\n",
    "    batch_size=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed4414d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_learned_policy(env, agent):\n",
    "    obs, _ = env.reset()\n",
    "    terminated, truncated = False, False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    \n",
    "    while not terminated and not truncated:\n",
    "        action = np.argmax(agent.q_table1.get(obs, np.zeros(env.action_space.n)) + agent.q_table2.get(obs, np.zeros(env.action_space.n)))\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "    \n",
    "    logging.info(\"Total Reward: %d, Steps: %d\", total_reward, steps)\n",
    "    return total_reward\n",
    "\n",
    "total_reward = run_learned_policy(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5821cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grid(env, agent, reward_across_episodes: list, epsilons_across_episodes: list) -> None:\n",
    "    env.train = False\n",
    "    total_reward_learned_policy = [run_learned_policy(env, agent) for _ in range(30)]\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    # Main plot\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(reward_across_episodes, 'ro')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward Value')\n",
    "    plt.title('Rewards Per Episode (Training)')\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(total_reward_learned_policy, 'ro')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward Value')\n",
    "    plt.title('Rewards Per Episode (Learned Policy Evaluation)')\n",
    "    plt.grid()\n",
    "\n",
    "    # Extra plots\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(reward_across_episodes)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Cumulative Reward Per Episode (Training)')\n",
    "    plt.title('Cumulative Reward vs Episode')\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(epsilons_across_episodes)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Epsilon Values')\n",
    "    plt.title('Epsilon Decay')\n",
    "    plt.grid()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_grid(stock_trading_environment, agent, reward_across_episodes, epsilons_across_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a3c9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_trading_environment = StockTradingEnvironment('./AAPL_data.csv', number_of_days_to_consider=30)\n",
    "stock_trading_environment.train = False\n",
    "obs, _ = stock_trading_environment.reset()\n",
    "terminated, truncated = False, False\n",
    "while not terminated:\n",
    "    action = np.argmax(agent.q_table1.get(obs, np.zeros(env.action_space.n)) + agent.q_table2.get(obs, np.zeros(env.action_space.n)))\n",
    "    obs, reward, terminated, truncated, info = stock_trading_environment.step(action)\n",
    "\n",
    "stock_trading_environment.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba8d1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle(agent, 'stock_env_q_learning_agent.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b416066",
   "metadata": {},
   "source": [
    "#### Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2ec473",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = load_pickle(\"stock_env_q_learning_agent.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e23d8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_trading_environment = StockTradingEnvironment('./AAPL_data.csv', number_of_days_to_consider=10)\n",
    "stock_trading_environment.train = False\n",
    "obs, _ = stock_trading_environment.reset()\n",
    "terminated, truncated = False, False\n",
    "while not terminated:\n",
    "    action = np.argmax(agent.q_table1.get(np.argmax(obs), np.zeros(env.action_space.n)) + agent.q_table2.get(np.argmax(obs), np.zeros(env.action_space.n)))\n",
    "    obs, reward, terminated, truncated, info = stock_trading_environment.step(action)\n",
    "\n",
    "stock_trading_environment.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
