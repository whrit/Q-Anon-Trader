{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f1cf939",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-23 14:49:49,944 - INFO - Using GPU for training\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "from src.environment import StockTradingEnvironment\n",
    "from src.utils import save_pickle, load_pickle, plot_grid\n",
    "from src.learner import q_learning_learning_loop\n",
    "import yfinance as yf\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import logging\n",
    "import multiprocessing as mp\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    logging.info(\"Using GPU for training\")\n",
    "else:\n",
    "    logging.info(\"Using CPU for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169c51c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yfinance download data\n",
    "def fetch_stock_data(symbol, start_date, end_date, output_file):\n",
    "    stock_data = yf.download(symbol, start=start_date, end=end_date)\n",
    "    stock_data['Close'] = stock_data['Adj Close']\n",
    "    \n",
    "    stock_data = stock_data.drop(columns=['Adj Close'])\n",
    "\n",
    "    stock_data.to_csv(output_file)\n",
    "        \n",
    "    return stock_data\n",
    "\n",
    "stock_data = fetch_stock_data('AAPL', '2019-06-20', '2024-06-21', 'AAPL_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bd4c31",
   "metadata": {},
   "source": [
    "#### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b1cb59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTree:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "        self.write = 0\n",
    "\n",
    "    def _propagate(self, idx, change):\n",
    "        parent = (idx - 1) // 2\n",
    "        self.tree[parent] += change\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    def _retrieve(self, idx, s):\n",
    "        left = 2 * idx + 1\n",
    "        right = left + 1\n",
    "        if left >= len(self.tree):\n",
    "            return idx\n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            return self._retrieve(right, s - self.tree[left])\n",
    "\n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "    def add(self, p, data):\n",
    "        idx = self.write + self.capacity - 1\n",
    "        self.data[self.write] = data\n",
    "        self.update(idx, p)\n",
    "        self.write += 1\n",
    "        if self.write >= self.capacity:\n",
    "            self.write = 0\n",
    "\n",
    "    def update(self, idx, p):\n",
    "        change = p - self.tree[idx]\n",
    "        self.tree[idx] = p\n",
    "        self._propagate(idx, change)\n",
    "\n",
    "    def get(self, s):\n",
    "        idx = self._retrieve(0, s)\n",
    "        dataIdx = idx - self.capacity + 1\n",
    "        return idx, self.tree[idx], self.data[dataIdx]\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=0.6):\n",
    "        self.tree = SumTree(capacity)\n",
    "        self.alpha = alpha\n",
    "        self.capacity = capacity\n",
    "        self.epsilon = 0.01\n",
    "\n",
    "    def add(self, error, sample):\n",
    "        p = (error + self.epsilon) ** self.alpha\n",
    "        self.tree.add(p, sample)\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        batch = []\n",
    "        idxs = []\n",
    "        segment = self.tree.total() / batch_size\n",
    "        priorities = []\n",
    "        for i in range(batch_size):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "            s = random.uniform(a, b)\n",
    "            idx, p, data = self.tree.get(s)\n",
    "            batch.append(data)\n",
    "            idxs.append(idx)\n",
    "            priorities.append(p)\n",
    "        sampling_probabilities = priorities / self.tree.total()\n",
    "        is_weights = np.power(self.tree.capacity * sampling_probabilities, -beta)\n",
    "        is_weights /= is_weights.max()\n",
    "        return batch, idxs, is_weights\n",
    "\n",
    "    def update(self, idx, error):\n",
    "        p = (error + self.epsilon) ** self.alpha\n",
    "        self.tree.update(idx, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e5f3ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleQLearningAgent:\n",
    "    def __init__(self, env, learning_rate, discount_factor):\n",
    "        self.env = env\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.q_table1 = {}\n",
    "        self.q_table2 = {}\n",
    "\n",
    "    def step(self, state, epsilon):\n",
    "        state_index = self._get_state_index(state)\n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.choice(self.env.action_space.n)\n",
    "        else:\n",
    "            q_values = self.q_table1.get(state_index, np.zeros(self.env.action_space.n)) + self.q_table2.get(state_index, np.zeros(self.env.action_space.n))\n",
    "            return np.argmax(q_values)\n",
    "\n",
    "    def update_qvalue(self, state, action, reward, next_state):\n",
    "        state_index = self._get_state_index(state)\n",
    "        next_state_index = self._get_state_index(next_state)\n",
    "        if np.random.rand() < 0.5:\n",
    "            best_next_action = np.argmax(self.q_table1.get(next_state_index, np.zeros(self.env.action_space.n)))\n",
    "            self.q_table1[state_index] = self.q_table1.get(state_index, np.zeros(self.env.action_space.n))\n",
    "            self.q_table1[state_index][action] += self.learning_rate * (\n",
    "                reward + self.discount_factor * self.q_table2.get(next_state_index, np.zeros(self.env.action_space.n))[best_next_action] - self.q_table1[state_index][action]\n",
    "            )\n",
    "        else:\n",
    "            best_next_action = np.argmax(self.q_table2.get(next_state_index, np.zeros(self.env.action_space.n)))\n",
    "            self.q_table2[state_index] = self.q_table2.get(state_index, np.zeros(self.env.action_space.n))\n",
    "            self.q_table2[state_index][action] += self.learning_rate * (\n",
    "                reward + self.discount_factor * self.q_table1.get(next_state_index, np.zeros(self.env.action_space.n))[best_next_action] - self.q_table2[state_index][action]\n",
    "            )\n",
    "\n",
    "    def _get_state_index(self, state):\n",
    "        return tuple(state.flatten())\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, env, learning_rate, discount_factor, buffer_size=10000, batch_size=32, alpha=0.6, beta=0.4):\n",
    "        self.env = env\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = PrioritizedReplayBuffer(buffer_size, alpha)\n",
    "        self.beta = beta\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        model = models.Sequential()\n",
    "        input_dim = np.prod(self.env.observation_space.shape)\n",
    "        model.add(layers.Dense(24, input_dim=input_dim, activation='relu'))\n",
    "        model.add(layers.Dense(24, activation='relu'))\n",
    "        model.add(layers.Dense(self.env.action_space.n, activation='linear'))\n",
    "        model.compile(optimizer=optimizers.Adam(learning_rate=self.learning_rate), loss='mse')\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        state = state.flatten().reshape(1, -1)\n",
    "        next_state = next_state.flatten().reshape(1, -1)\n",
    "        target = reward + self.discount_factor * np.amax(self.target_model.predict(next_state)[0]) if not done else reward\n",
    "        td_error = abs(target - np.amax(self.model.predict(state)[0]))\n",
    "        self.memory.add(td_error, (state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state, epsilon):\n",
    "        if np.random.rand() <= epsilon:\n",
    "            return np.random.choice(self.env.action_space.n)\n",
    "        state = state.flatten().reshape(1, -1)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory.tree.data) < self.batch_size:\n",
    "            return\n",
    "        minibatch, idxs, is_weights = self.memory.sample(self.batch_size, self.beta)\n",
    "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "        states = np.array(states).reshape(self.batch_size, -1)\n",
    "        next_states = np.array(next_states).reshape(self.batch_size, -1)\n",
    "        targets = self.model.predict(states)\n",
    "        targets_next = self.target_model.predict(next_states)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            target = rewards[i] + self.discount_factor * np.amax(targets_next[i]) if not dones[i] else rewards[i]\n",
    "            td_error = abs(target - targets[i][actions[i]])\n",
    "            self.memory.update(idxs[i], td_error)\n",
    "            targets[i][actions[i]] = target\n",
    "\n",
    "        self.model.fit(states, targets, epochs=1, verbose=0)\n",
    "        self.update_target_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22706b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_experience(env, agent, epsilon):\n",
    "    obs, _ = env.reset()\n",
    "    terminated, truncated = False, False\n",
    "    experiences = []\n",
    "\n",
    "    while not terminated and not truncated:\n",
    "        current_action = agent.step(obs, epsilon)\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(current_action)\n",
    "        experiences.append((obs, current_action, reward, next_obs, terminated))\n",
    "        obs = next_obs\n",
    "    \n",
    "    return experiences\n",
    "\n",
    "def adaptive_learning_rate(epoch, lr):\n",
    "    if epoch % 100 == 0 and epoch:\n",
    "        return lr * 0.1\n",
    "    return lr\n",
    "\n",
    "def decay_epsilon(episode, epsilon, min_epsilon, decay_rate):\n",
    "    return max(min_epsilon, epsilon * decay_rate)\n",
    "\n",
    "def q_learning_learning_loop(env, agent, learning_rate: float, discount_factor: float, episodes: int,\n",
    "                             min_epsilon_allowed: float, initial_epsilon_value: float,\n",
    "                             buffer_size: int = 10000, batch_size: int = 32, decay_method=\"exponential\") -> tuple:\n",
    "    epsilon = initial_epsilon_value\n",
    "    epsilon_decay_factor = np.power(min_epsilon_allowed / epsilon, 1 / episodes)\n",
    "\n",
    "    reward_across_episodes = []\n",
    "    epsilons_across_episodes = []\n",
    "\n",
    "    replay_buffer = deque(maxlen=buffer_size)\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        experiences = collect_experience(env, agent, epsilon)\n",
    "        replay_buffer.extend(experiences)\n",
    "\n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            batch = np.array(random.sample(replay_buffer, batch_size), dtype=object)\n",
    "            for b_state, b_action, b_reward, b_next_state, b_done in batch:\n",
    "                b_state = tuple(b_state.flatten())  # Convert to tuple\n",
    "                b_next_state = tuple(b_next_state.flatten())  # Convert to tuple\n",
    "                if b_done:\n",
    "                    target = b_reward\n",
    "                else:\n",
    "                    target = b_reward + discount_factor * np.max(agent.q_table1.get(b_next_state, np.zeros(env.action_space.n)) + agent.q_table2.get(b_next_state, np.zeros(env.action_space.n)))\n",
    "\n",
    "                agent.q_table1[b_state] = agent.q_table1.get(b_state, np.zeros(env.action_space.n))\n",
    "                agent.q_table1[b_state][b_action] += learning_rate * (target - agent.q_table1[b_state][b_action])\n",
    "\n",
    "        if decay_method == \"exponential\":\n",
    "            epsilon = decay_epsilon(episode, epsilon, min_epsilon_allowed, epsilon_decay_factor)\n",
    "        else:\n",
    "            epsilon = max(min_epsilon_allowed, epsilon - (initial_epsilon_value - min_epsilon_allowed) / episodes)\n",
    "        \n",
    "        reward_across_episodes.append(sum([exp[2] for exp in experiences]))\n",
    "        epsilons_across_episodes.append(epsilon)\n",
    "\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            logging.info(f\"Episode {episode + 1}/{episodes} - Reward: {reward_across_episodes[-1]} - Epsilon: {epsilon}\")\n",
    "\n",
    "    logging.info(\"Trained Q-Table: %s\", agent.q_table1)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(reward_across_episodes, label='Rewards')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Cumulative Reward')\n",
    "    plt.title('Rewards over Episodes')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epsilons_across_episodes, label='Epsilon')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Epsilon')\n",
    "    plt.title('Epsilon Decay over Episodes')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return agent, reward_across_episodes, epsilons_across_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d70e1189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_collect_experience(env, agent, epsilon, num_workers=4):\n",
    "    with mp.Pool(num_workers) as pool:\n",
    "        results = pool.starmap(collect_experience, [(env, agent, epsilon) for _ in range(num_workers)])\n",
    "    experiences = [exp for result in results for exp in result]\n",
    "    return experiences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca06c6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: You can adjust the parameter 'number_of_days_to_consider'\n",
    "\n",
    "env = stock_trading_environment = StockTradingEnvironment('./AAPL_data.csv', number_of_days_to_consider=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4d38c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-23 14:51:05,462 - INFO - Episode 100/50000 - Reward: -1585.7576360484668 - Epsilon: 0.9908319448927725\n",
      "2024-06-23 14:52:03,918 - INFO - Episode 200/50000 - Reward: -1554.3286083740397 - Epsilon: 0.9817479430199942\n",
      "2024-06-23 14:53:04,011 - INFO - Episode 300/50000 - Reward: -1688.6333644746494 - Epsilon: 0.9727472237769801\n",
      "2024-06-23 14:54:03,581 - INFO - Episode 400/50000 - Reward: -1752.4343927235489 - Epsilon: 0.9638290236239909\n",
      "2024-06-23 14:55:02,515 - INFO - Episode 500/50000 - Reward: -1398.4896248442853 - Epsilon: 0.954992586021461\n",
      "2024-06-23 14:56:02,025 - INFO - Episode 600/50000 - Reward: -1680.3684293335418 - Epsilon: 0.9462371613658228\n",
      "2024-06-23 14:57:00,873 - INFO - Episode 700/50000 - Reward: -1573.8371964666544 - Epsilon: 0.937562006925914\n",
      "2024-06-23 14:57:59,970 - INFO - Episode 800/50000 - Reward: -1556.4228127668825 - Epsilon: 0.9289663867799748\n",
      "2024-06-23 14:58:59,182 - INFO - Episode 900/50000 - Reward: -1631.7386836587145 - Epsilon: 0.9204495717532143\n",
      "2024-06-23 14:59:57,624 - INFO - Episode 1000/50000 - Reward: -1587.8794432223528 - Epsilon: 0.9120108393559571\n",
      "2024-06-23 15:00:56,750 - INFO - Episode 1100/50000 - Reward: -1618.04304012745 - Epsilon: 0.9036494737223535\n",
      "2024-06-23 15:01:57,222 - INFO - Episode 1200/50000 - Reward: -1606.7930420632777 - Epsilon: 0.8953647655496506\n",
      "2024-06-23 15:02:57,558 - INFO - Episode 1300/50000 - Reward: -1686.3251107122169 - Epsilon: 0.8871560120380219\n",
      "2024-06-23 15:03:57,494 - INFO - Episode 1400/50000 - Reward: -1723.8787542091604 - Epsilon: 0.8790225168309495\n",
      "2024-06-23 15:04:57,254 - INFO - Episode 1500/50000 - Reward: -1704.0654668245372 - Epsilon: 0.8709635899561496\n",
      "2024-06-23 15:05:57,810 - INFO - Episode 1600/50000 - Reward: -1658.3150413239748 - Epsilon: 0.8629785477670433\n",
      "2024-06-23 15:07:00,174 - INFO - Episode 1700/50000 - Reward: -1656.8896874393167 - Epsilon: 0.8550667128847602\n",
      "2024-06-23 15:08:00,869 - INFO - Episode 1800/50000 - Reward: -1720.8730175403025 - Epsilon: 0.847227414140677\n",
      "2024-06-23 15:09:02,018 - INFO - Episode 1900/50000 - Reward: -1815.9923754423053 - Epsilon: 0.8394599865194816\n",
      "2024-06-23 15:10:05,647 - INFO - Episode 2000/50000 - Reward: -1562.5007269127502 - Epsilon: 0.831763771102759\n",
      "2024-06-23 15:11:06,723 - INFO - Episode 2100/50000 - Reward: -1679.826632407566 - Epsilon: 0.824138115013094\n",
      "2024-06-23 15:12:09,109 - INFO - Episode 2200/50000 - Reward: -1813.0122064269851 - Epsilon: 0.8165823713586867\n",
      "2024-06-23 15:13:11,238 - INFO - Episode 2300/50000 - Reward: -1738.1424344362 - Epsilon: 0.8090958991784798\n",
      "2024-06-23 15:14:12,987 - INFO - Episode 2400/50000 - Reward: -1800.4302002172337 - Epsilon: 0.8016780633877798\n",
      "2024-06-23 15:15:14,427 - INFO - Episode 2500/50000 - Reward: -1697.7258564100082 - Epsilon: 0.7943282347243855\n",
      "2024-06-23 15:16:17,503 - INFO - Episode 2600/50000 - Reward: -1890.9324451687114 - Epsilon: 0.7870457896952061\n",
      "2024-06-23 15:17:20,249 - INFO - Episode 2700/50000 - Reward: -1698.8006626444374 - Epsilon: 0.779830110523369\n",
      "2024-06-23 15:18:26,107 - INFO - Episode 2800/50000 - Reward: -1709.1745508735974 - Epsilon: 0.7726805850958163\n",
      "2024-06-23 15:19:29,974 - INFO - Episode 2900/50000 - Reward: -1837.830159261641 - Epsilon: 0.7655966069113732\n",
      "2024-06-23 15:20:38,775 - INFO - Episode 3000/50000 - Reward: -1688.5454433973562 - Epsilon: 0.7585775750293033\n",
      "2024-06-23 15:21:43,128 - INFO - Episode 3100/50000 - Reward: -1875.9794450694123 - Epsilon: 0.7516228940183277\n",
      "2024-06-23 15:23:00,315 - INFO - Episode 3200/50000 - Reward: -1766.928815073963 - Epsilon: 0.7447319739061141\n",
      "2024-06-23 15:24:10,549 - INFO - Episode 3300/50000 - Reward: -1864.2727547922516 - Epsilon: 0.7379042301292285\n",
      "2024-06-23 15:25:27,265 - INFO - Episode 3400/50000 - Reward: -2048.071946314675 - Epsilon: 0.7311390834835477\n",
      "2024-06-23 15:26:37,215 - INFO - Episode 3500/50000 - Reward: -1947.3163320354638 - Epsilon: 0.7244359600751226\n",
      "2024-06-23 15:27:58,767 - INFO - Episode 3600/50000 - Reward: -1894.5391648111474 - Epsilon: 0.7177942912714969\n",
      "2024-06-23 15:29:20,087 - INFO - Episode 3700/50000 - Reward: -1860.243325909828 - Epsilon: 0.7112135136534666\n",
      "2024-06-23 15:30:32,171 - INFO - Episode 3800/50000 - Reward: -2191.0608700516646 - Epsilon: 0.7046930689672868\n",
      "2024-06-23 15:31:51,045 - INFO - Episode 3900/50000 - Reward: -2035.962904597659 - Epsilon: 0.6982324040773131\n",
      "2024-06-23 15:33:08,674 - INFO - Episode 4000/50000 - Reward: -2140.711577237369 - Epsilon: 0.6918309709190805\n",
      "2024-06-23 15:34:22,698 - INFO - Episode 4100/50000 - Reward: -1798.4898023103697 - Epsilon: 0.685488226452808\n",
      "2024-06-23 15:35:37,951 - INFO - Episode 4200/50000 - Reward: -2097.218484943341 - Epsilon: 0.6792036326173337\n",
      "2024-06-23 15:37:06,971 - INFO - Episode 4300/50000 - Reward: -1854.5103985205924 - Epsilon: 0.6729766562844688\n",
      "2024-06-23 15:38:50,877 - INFO - Episode 4400/50000 - Reward: -2015.0615908227508 - Epsilon: 0.6668067692137751\n",
      "2024-06-23 15:41:06,570 - INFO - Episode 4500/50000 - Reward: -2121.584210658114 - Epsilon: 0.6606934480077508\n",
      "2024-06-23 15:43:15,621 - INFO - Episode 4600/50000 - Reward: -1841.317541428876 - Epsilon: 0.654636174067431\n",
      "2024-06-23 15:45:23,410 - INFO - Episode 4700/50000 - Reward: -1837.5950467213443 - Epsilon: 0.6486344335483964\n",
      "2024-06-23 15:53:11,787 - INFO - Episode 4800/50000 - Reward: -1864.9609069001997 - Epsilon: 0.6426877173171793\n",
      "2024-06-23 15:54:24,695 - INFO - Episode 4900/50000 - Reward: -2026.5205772912075 - Epsilon: 0.6367955209080773\n",
      "2024-06-23 16:02:49,769 - INFO - Episode 5000/50000 - Reward: -2119.846397776878 - Epsilon: 0.6309573444803569\n",
      "2024-06-23 16:12:33,552 - INFO - Episode 5100/50000 - Reward: -2119.706802701628 - Epsilon: 0.6251726927758517\n",
      "2024-06-23 16:23:06,651 - INFO - Episode 5200/50000 - Reward: -1826.7880487525872 - Epsilon: 0.6194410750769493\n",
      "2024-06-23 16:31:53,003 - INFO - Episode 5300/50000 - Reward: -2190.5655567027734 - Epsilon: 0.6137620051649637\n",
      "2024-06-23 16:40:43,407 - INFO - Episode 5400/50000 - Reward: -2233.946296759787 - Epsilon: 0.6081350012788888\n",
      "2024-06-23 16:47:49,448 - INFO - Episode 5500/50000 - Reward: -2155.874588943181 - Epsilon: 0.6025595860745301\n",
      "2024-06-23 16:50:56,495 - INFO - Episode 5600/50000 - Reward: -2118.4662474314 - Epsilon: 0.5970352865840113\n",
      "2024-06-23 17:00:56,059 - INFO - Episode 5700/50000 - Reward: -2015.1360963472162 - Epsilon: 0.59156163417565\n",
      "2024-06-23 17:07:11,077 - INFO - Episode 5800/50000 - Reward: -2420.3989104204647 - Epsilon: 0.5861381645142064\n",
      "2024-06-23 17:09:50,519 - INFO - Episode 5900/50000 - Reward: -2415.976925972002 - Epsilon: 0.5807644175214915\n",
      "2024-06-23 17:15:54,668 - INFO - Episode 6000/50000 - Reward: -2121.959268993552 - Epsilon: 0.5754399373373372\n",
      "2024-06-23 17:24:24,462 - INFO - Episode 6100/50000 - Reward: -2256.7893605583613 - Epsilon: 0.5701642722809294\n",
      "2024-06-23 18:08:01,027 - INFO - Episode 6200/50000 - Reward: -2167.392166297419 - Epsilon: 0.5649369748124854\n",
      "2024-06-23 18:11:54,299 - INFO - Episode 6300/50000 - Reward: -2195.5127695776823 - Epsilon: 0.5597576014952944\n",
      "2024-06-23 18:23:21,342 - INFO - Episode 6400/50000 - Reward: -2142.9331070794283 - Epsilon: 0.5546257129580962\n",
      "2024-06-23 18:34:31,348 - INFO - Episode 6500/50000 - Reward: -2344.354793993397 - Epsilon: 0.5495408738578113\n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x10367f730>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/beckett/miniconda3/envs/tradeEnv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "# Example with a higher batch size of 256\n",
    "agent = DoubleQLearningAgent(env, learning_rate=0.001, discount_factor=0.95)\n",
    "\n",
    "agent, reward_across_episodes, epsilons_across_episodes = q_learning_learning_loop(\n",
    "    env,\n",
    "    agent,\n",
    "    learning_rate=0.001,\n",
    "    discount_factor=0.95,\n",
    "    episodes=50000,\n",
    "    min_epsilon_allowed=0.01,\n",
    "    initial_epsilon_value=1,\n",
    "    buffer_size=100000,\n",
    "    batch_size=256,  # Increased batch size\n",
    "    decay_method=\"exponential\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed4414d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_learned_policy(env, agent):\n",
    "    obs, _ = env.reset()\n",
    "    terminated, truncated = False, False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "\n",
    "    while not terminated and not truncated:\n",
    "        obs_tuple = tuple(obs.flatten())\n",
    "        action = np.argmax(agent.q_table1.get(obs_tuple, np.zeros(env.action_space.n)) + agent.q_table2.get(obs_tuple, np.zeros(env.action_space.n)))\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "\n",
    "    logging.info(\"Total Reward: %d, Steps: %d\", total_reward, steps)\n",
    "    return total_reward\n",
    "\n",
    "total_reward = run_learned_policy(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5821cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grid(env, agent, reward_across_episodes: list, epsilons_across_episodes: list) -> None:\n",
    "    env.train = False\n",
    "    total_reward_learned_policy = [run_learned_policy(env, agent) for _ in range(30)]\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    # Main plot\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(reward_across_episodes, 'ro')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward Value')\n",
    "    plt.title('Rewards Per Episode (Training)')\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(total_reward_learned_policy, 'ro')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward Value')\n",
    "    plt.title('Rewards Per Episode (Learned Policy Evaluation)')\n",
    "    plt.grid()\n",
    "\n",
    "    # Extra plots\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(reward_across_episodes)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Cumulative Reward Per Episode (Training)')\n",
    "    plt.title('Cumulative Reward vs Episode')\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(epsilons_across_episodes)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Epsilon Values')\n",
    "    plt.title('Epsilon Decay')\n",
    "    plt.grid()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_grid(stock_trading_environment, agent, reward_across_episodes, epsilons_across_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a3c9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_trading_environment = StockTradingEnvironment('./AAPL_data.csv', number_of_days_to_consider=30)\n",
    "stock_trading_environment.train = False\n",
    "obs, _ = stock_trading_environment.reset()\n",
    "terminated, truncated = False, False\n",
    "while not terminated:\n",
    "    # Convert observation to a tuple\n",
    "    obs_tuple = tuple(obs.flatten())\n",
    "    action = np.argmax(agent.q_table1.get(obs_tuple, np.zeros(env.action_space.n)) + agent.q_table2.get(obs_tuple, np.zeros(env.action_space.n)))\n",
    "    obs, reward, terminated, truncated, info = stock_trading_environment.step(action)\n",
    "\n",
    "stock_trading_environment.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba8d1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle(agent, 'aapl_q_learning_agent.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b416066",
   "metadata": {},
   "source": [
    "#### Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2ec473",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = load_pickle(\"aapl_q_learning_agent.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e23d8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_trading_environment = StockTradingEnvironment('./AAPL_data.csv', number_of_days_to_consider=30)\n",
    "stock_trading_environment.train = False\n",
    "obs, _ = stock_trading_environment.reset()\n",
    "terminated, truncated = False, False\n",
    "while not terminated:\n",
    "    # Convert observation to a tuple\n",
    "    obs_tuple = tuple(obs.flatten())\n",
    "    action = np.argmax(agent.q_table1.get(obs_tuple, np.zeros(env.action_space.n)) + agent.q_table2.get(obs_tuple, np.zeros(env.action_space.n)))\n",
    "    obs, reward, terminated, truncated, info = stock_trading_environment.step(action)\n",
    "\n",
    "stock_trading_environment.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
