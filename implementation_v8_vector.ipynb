{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f1cf939",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-24 20:39:58,787 - INFO - Using GPU for training\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "from src.environment import StockTradingEnvironment, make_env\n",
    "from src.utils import save_pickle, load_pickle, plot_grid\n",
    "import gymnasium as gym\n",
    "import gym_trading_env\n",
    "import yfinance as yf\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import logging\n",
    "import multiprocessing as mp\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    device = '/GPU:0'\n",
    "    logging.info(\"Using GPU for training\")\n",
    "else:\n",
    "    device = '/CPU:0'\n",
    "    logging.info(\"Using CPU for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169c51c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_stock_data(symbol, start_date, end_date, output_file):\n",
    "    stock_data = yf.download(symbol, start=start_date, end=end_date)\n",
    "    stock_data['Close'] = stock_data['Adj Close']\n",
    "    \n",
    "    stock_data = stock_data.drop(columns=['Adj Close'])\n",
    "\n",
    "    # Convert column headers to lowercase\n",
    "    stock_data.columns = stock_data.columns.str.lower()\n",
    "\n",
    "    stock_data.to_csv(output_file)\n",
    "        \n",
    "    return stock_data\n",
    "\n",
    "stock_data = fetch_stock_data('AAPL', '2019-06-20', '2024-06-21', 'AAPL_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a42954e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_loss(y_true, y_pred, clip_delta=1.0):\n",
    "    error = y_true - y_pred\n",
    "    cond = K.abs(error) <= clip_delta\n",
    "    squared_loss = 0.5 * K.square(error)\n",
    "    quadratic_loss = 0.5 * K.square(clip_delta) + clip_delta * (K.abs(error) - clip_delta)\n",
    "    return K.mean(tf.where(cond, squared_loss, quadratic_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bd4c31",
   "metadata": {},
   "source": [
    "#### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b1cb59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTree:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "        self.write = 0\n",
    "\n",
    "    def _propagate(self, idx, change):\n",
    "        parent = (idx - 1) // 2\n",
    "        self.tree[parent] += change\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    def _retrieve(self, idx, s):\n",
    "        left = 2 * idx + 1\n",
    "        right = left + 1\n",
    "        if left >= len(self.tree):\n",
    "            return idx\n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            return self._retrieve(right, s - self.tree[left])\n",
    "\n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "    def add(self, p, data):\n",
    "        idx = self.write + self.capacity - 1\n",
    "        self.data[self.write] = data\n",
    "        self.update(idx, p)\n",
    "        self.write += 1\n",
    "        if self.write >= self.capacity:\n",
    "            self.write = 0\n",
    "\n",
    "    def update(self, idx, p):\n",
    "        change = p - self.tree[idx]\n",
    "        self.tree[idx] = p\n",
    "        self._propagate(idx, change)\n",
    "\n",
    "    def get(self, s):\n",
    "        idx = self._retrieve(0, s)\n",
    "        dataIdx = idx - self.capacity + 1\n",
    "        return idx, self.tree[idx], self.data[dataIdx]\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=0.6):\n",
    "        self.tree = SumTree(capacity)\n",
    "        self.alpha = alpha\n",
    "        self.capacity = capacity\n",
    "        self.epsilon = 0.01\n",
    "\n",
    "    def add(self, error, sample):\n",
    "        p = (error + self.epsilon) ** self.alpha\n",
    "        self.tree.add(p, sample)\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        batch = []\n",
    "        idxs = []\n",
    "        segment = self.tree.total() / batch_size\n",
    "        priorities = []\n",
    "        for i in range(batch_size):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "            s = random.uniform(a, b)\n",
    "            idx, p, data = self.tree.get(s)\n",
    "            batch.append(data)\n",
    "            idxs.append(idx)\n",
    "            priorities.append(p)\n",
    "        sampling_probabilities = priorities / self.tree.total()\n",
    "        is_weights = np.power(self.tree.capacity * sampling_probabilities, -beta)\n",
    "        is_weights /= is_weights.max()\n",
    "        return batch, idxs, is_weights\n",
    "\n",
    "    def update(self, idx, error):\n",
    "        p = (error + self.epsilon) ** self.alpha\n",
    "        self.tree.update(idx, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e5f3ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleQLearningAgent:\n",
    "    def __init__(self, env, learning_rate, discount_factor, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995):\n",
    "        self.env = env\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.q_table1 = {}\n",
    "        self.q_table2 = {}\n",
    "\n",
    "    def step(self, state):\n",
    "        state_index = self._get_state_index(state)\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.env.action_space.n)\n",
    "        else:\n",
    "            q_values = self.q_table1.get(state_index, np.zeros(self.env.action_space.n)) + self.q_table2.get(state_index, np.zeros(self.env.action_space.n))\n",
    "            return np.argmax(q_values)\n",
    "\n",
    "    def update_qvalue(self, state, action, reward, next_state):\n",
    "        state_index = self._get_state_index(state)\n",
    "        next_state_index = self._get_state_index(next_state)\n",
    "        if np.random.rand() < 0.5:\n",
    "            best_next_action = np.argmax(self.q_table1.get(next_state_index, np.zeros(self.env.action_space.n)))\n",
    "            self.q_table1[state_index] = self.q_table1.get(state_index, np.zeros(self.env.action_space.n))\n",
    "            self.q_table1[state_index][action] += self.learning_rate * (\n",
    "                reward + self.discount_factor * self.q_table2.get(next_state_index, np.zeros(self.env.action_space.n))[best_next_action] - self.q_table1[state_index][action]\n",
    "            )\n",
    "        else:\n",
    "            best_next_action = np.argmax(self.q_table2.get(next_state_index, np.zeros(self.env.action_space.n)))\n",
    "            self.q_table2[state_index] = self.q_table2.get(state_index, np.zeros(self.env.action_space.n))\n",
    "            self.q_table2[state_index][action] += self.learning_rate * (\n",
    "                reward + self.discount_factor * self.q_table1.get(next_state_index, np.zeros(self.env.action_space.n))[best_next_action] - self.q_table2[state_index][action]\n",
    "            )\n",
    "\n",
    "    def _get_state_index(self, state):\n",
    "        return tuple(state.flatten())\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# DQN Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, env, model_name='DQN_model', gamma=0.95, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995, learning_rate=0.001):\n",
    "        self.env = env\n",
    "        self.model_name = model_name\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss = huber_loss\n",
    "        self.custom_objects = {\"huber_loss\": huber_loss}\n",
    "        self.optimizer = Adam(learning_rate=self.learning_rate)\n",
    "        self.buffer_size = 10000\n",
    "        self.batch_size = 32\n",
    "        self.memory = PrioritizedReplayBuffer(self.buffer_size, 0.6)\n",
    "        self.beta = 0.4\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units=128, activation=\"relu\", input_dim=self.env.observation_space.shape[1]))\n",
    "        model.add(Dense(units=256, activation=\"relu\"))\n",
    "        model.add(Dense(units=256, activation=\"relu\"))\n",
    "        model.add(Dense(units=128, activation=\"relu\"))\n",
    "        model.add(Dense(units=self.env.action_space.n))\n",
    "        model.compile(loss=self.loss, optimizer=self.optimizer)\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        state = state.flatten().reshape(1, -1)\n",
    "        next_state = next_state.flatten().reshape(1, -1)\n",
    "        target = reward + self.gamma * np.amax(self.target_model.predict(next_state)[0]) if not done else reward\n",
    "        td_error = abs(target - np.amax(self.model.predict(state)[0]))\n",
    "        self.memory.add(td_error, (state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.env.action_space.n)\n",
    "        state = state.flatten().reshape(1, -1)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory.tree.data) < self.batch_size:\n",
    "            return\n",
    "        minibatch, idxs, is_weights = self.memory.sample(self.batch_size, self.beta)\n",
    "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "        states = np.array(states).reshape(self.batch_size, -1)\n",
    "        next_states = np.array(next_states).reshape(self.batch_size, -1)\n",
    "        targets = self.model.predict(states)\n",
    "        targets_next = self.target_model.predict(next_states)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            target = rewards[i] + self.gamma * np.amax(targets_next[i]) if not dones[i] else rewards[i]\n",
    "            td_error = abs(target - targets[i][actions[i]])\n",
    "            self.memory.update(idxs[i], td_error)\n",
    "            targets[i][actions[i]] = target\n",
    "\n",
    "        self.model.fit(states, targets, epochs=1, verbose=0)\n",
    "        self.update_target_model()\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22706b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_learning_loop(env, agent, learning_rate: float, discount_factor: float, episodes: int,\n",
    "                             min_epsilon_allowed: float, initial_epsilon_value: float,\n",
    "                             buffer_size: int = 10000, batch_size: int = 32, decay_method=\"exponential\") -> tuple:\n",
    "    epsilon = initial_epsilon_value\n",
    "    epsilon_decay_factor = np.power(min_epsilon_allowed / epsilon, 1 / episodes)\n",
    "\n",
    "    reward_across_episodes = []\n",
    "    epsilons_across_episodes = []\n",
    "\n",
    "    replay_buffer = deque(maxlen=buffer_size)\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        observations = env.reset()\n",
    "        terminated = np.array([False] * env.num_envs)\n",
    "        truncated = np.array([False] * env.num_envs)\n",
    "        episode_rewards = np.zeros(env.num_envs)\n",
    "        \n",
    "        while not np.all(terminated) and not np.all(truncated):\n",
    "            actions = [agent.step(observation) for observation in observations]\n",
    "            next_observations, rewards, terminated, truncated, _ = env.step(actions)\n",
    "            episode_rewards += rewards\n",
    "            \n",
    "            for obs, action, reward, next_obs, term, trun in zip(observations, actions, rewards, next_observations, terminated, truncated):\n",
    "                replay_buffer.append((obs, action, reward, next_obs, term or trun))\n",
    "                \n",
    "            observations = next_observations\n",
    "\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                batch = np.array(random.sample(replay_buffer, batch_size), dtype=object)\n",
    "                for b_state, b_action, b_reward, b_next_state, b_done in batch:\n",
    "                    b_state = tuple(b_state.flatten())\n",
    "                    b_next_state = tuple(b_next_state.flatten())\n",
    "                    if b_done:\n",
    "                        target = b_reward\n",
    "                    else:\n",
    "                        target = b_reward + discount_factor * np.max(agent.q_table1.get(b_next_state, np.zeros(env.action_space.n)) + agent.q_table2.get(b_next_state, np.zeros(env.action_space.n)))\n",
    "\n",
    "                    agent.q_table1[b_state] = agent.q_table1.get(b_state, np.zeros(env.action_space.n))\n",
    "                    agent.q_table1[b_state][b_action] += learning_rate * (target - agent.q_table1[b_state][b_action])\n",
    "\n",
    "        if decay_method == \"exponential\":\n",
    "            epsilon = max(min_epsilon_allowed, epsilon * epsilon_decay_factor)\n",
    "        else:\n",
    "            epsilon = max(min_epsilon_allowed, epsilon - (initial_epsilon_value - min_epsilon_allowed) / episodes)\n",
    "\n",
    "        reward_across_episodes.append(np.mean(episode_rewards))\n",
    "        epsilons_across_episodes.append(epsilon)\n",
    "\n",
    "        agent.decay_epsilon()\n",
    "\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            logging.info(f\"Episode {episode + 1}/{episodes} - Mean Reward: {np.mean(episode_rewards)} - Epsilon: {epsilon}\")\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(reward_across_episodes, label='Rewards')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Mean Reward')\n",
    "    plt.title('Rewards over Episodes')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epsilons_across_episodes, label='Epsilon')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Epsilon')\n",
    "    plt.title('Epsilon Decay over Episodes')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return agent, reward_across_episodes, epsilons_across_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4d38c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame head:                  open       high        low      close     volume\n",
      "Date                                                             \n",
      "2019-06-20  50.092499  50.152500  49.507500  49.865002   86056000\n",
      "2019-06-21  49.700001  50.212502  49.537498  49.695000  191202400\n",
      "2019-06-24  49.634998  50.040001  49.542500  49.645000   72881600\n",
      "2019-06-25  49.607498  49.814999  48.822498  48.892502   84281200\n",
      "2019-06-26  49.442501  50.247501  49.337502  49.950001  104270000\n",
      "DataFrame with TA features head:                  open       high        low      close     volume  \\\n",
      "Date                                                                \n",
      "2019-06-20  50.092499  50.152500  49.507500  49.865002   86056000   \n",
      "2019-06-21  49.700001  50.212502  49.537498  49.695000  191202400   \n",
      "2019-06-24  49.634998  50.040001  49.542500  49.645000   72881600   \n",
      "2019-06-25  49.607498  49.814999  48.822498  48.892502   84281200   \n",
      "2019-06-26  49.442501  50.247501  49.337502  49.950001  104270000   \n",
      "\n",
      "              volume_adi  volume_obv  volume_cmf  volume_fi  volume_em  ...  \\\n",
      "Date                                                                    ...   \n",
      "2019-06-20  9.339872e+06    86056000         0.0        0.0   0.000000  ...   \n",
      "2019-06-21 -9.263445e+07  -105146400         0.0        0.0   0.015886  ...   \n",
      "2019-06-24 -1.354843e+08  -178028000         0.0        0.0  -0.057169  ...   \n",
      "2019-06-25 -2.078763e+08  -262309200         0.0        0.0  -0.556421  ...   \n",
      "2019-06-26 -1.717830e+08  -158039200         0.0        0.0   0.413460  ...   \n",
      "\n",
      "            momentum_ppo  momentum_ppo_signal  momentum_ppo_hist  \\\n",
      "Date                                                               \n",
      "2019-06-20           0.0                  0.0                0.0   \n",
      "2019-06-21           0.0                  0.0                0.0   \n",
      "2019-06-24           0.0                  0.0                0.0   \n",
      "2019-06-25           0.0                  0.0                0.0   \n",
      "2019-06-26           0.0                  0.0                0.0   \n",
      "\n",
      "            momentum_pvo  momentum_pvo_signal  momentum_pvo_hist  \\\n",
      "Date                                                               \n",
      "2019-06-20           0.0                  0.0                0.0   \n",
      "2019-06-21           0.0                  0.0                0.0   \n",
      "2019-06-24           0.0                  0.0                0.0   \n",
      "2019-06-25           0.0                  0.0                0.0   \n",
      "2019-06-26           0.0                  0.0                0.0   \n",
      "\n",
      "            momentum_kama  others_dr  others_dlr  others_cr  \n",
      "Date                                                         \n",
      "2019-06-20            0.0   0.000000    0.000000   0.000000  \n",
      "2019-06-21            0.0  -0.340924   -0.341507  -0.340924  \n",
      "2019-06-24            0.0  -0.100612   -0.100663  -0.441194  \n",
      "2019-06-25            0.0  -1.515759   -1.527364  -1.950265  \n",
      "2019-06-26            0.0   2.162906    2.139847   0.170458  \n",
      "\n",
      "[5 rows x 91 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/beckett/miniconda3/envs/tradeEnv/lib/python3.10/site-packages/ta/trend.py:1030: FutureWarning: Series.__setitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To set a value by position, use `ser.iloc[pos] = value`\n",
      "  self._psar[i] = high2\n",
      "/Users/beckett/miniconda3/envs/tradeEnv/lib/python3.10/site-packages/ta/trend.py:1030: FutureWarning: Series.__setitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To set a value by position, use `ser.iloc[pos] = value`\n",
      "  self._psar[i] = high2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns after adding technical indicators: Index(['open', 'high', 'low', 'close', 'volume', 'volume_adi', 'volume_obv',\n",
      "       'volume_cmf', 'volume_fi', 'volume_em', 'volume_sma_em', 'volume_vpt',\n",
      "       'volume_vwap', 'volume_mfi', 'volume_nvi', 'volatility_bbm',\n",
      "       'volatility_bbh', 'volatility_bbl', 'volatility_bbw', 'volatility_bbp',\n",
      "       'volatility_bbhi', 'volatility_bbli', 'volatility_kcc',\n",
      "       'volatility_kch', 'volatility_kcl', 'volatility_kcw', 'volatility_kcp',\n",
      "       'volatility_kchi', 'volatility_kcli', 'volatility_dcl',\n",
      "       'volatility_dch', 'volatility_dcm', 'volatility_dcw', 'volatility_dcp',\n",
      "       'volatility_atr', 'volatility_ui', 'trend_macd', 'trend_macd_signal',\n",
      "       'trend_macd_diff', 'trend_sma_fast', 'trend_sma_slow', 'trend_ema_fast',\n",
      "       'trend_ema_slow', 'trend_vortex_ind_pos', 'trend_vortex_ind_neg',\n",
      "       'trend_vortex_ind_diff', 'trend_trix', 'trend_mass_index', 'trend_dpo',\n",
      "       'trend_kst', 'trend_kst_sig', 'trend_kst_diff', 'trend_ichimoku_conv',\n",
      "       'trend_ichimoku_base', 'trend_ichimoku_a', 'trend_ichimoku_b',\n",
      "       'trend_stc', 'trend_adx', 'trend_adx_pos', 'trend_adx_neg', 'trend_cci',\n",
      "       'trend_visual_ichimoku_a', 'trend_visual_ichimoku_b', 'trend_aroon_up',\n",
      "       'trend_aroon_down', 'trend_aroon_ind', 'trend_psar_up',\n",
      "       'trend_psar_down', 'trend_psar_up_indicator',\n",
      "       'trend_psar_down_indicator', 'momentum_rsi', 'momentum_stoch_rsi',\n",
      "       'momentum_stoch_rsi_k', 'momentum_stoch_rsi_d', 'momentum_tsi',\n",
      "       'momentum_uo', 'momentum_stoch', 'momentum_stoch_signal', 'momentum_wr',\n",
      "       'momentum_ao', 'momentum_roc', 'momentum_ppo', 'momentum_ppo_signal',\n",
      "       'momentum_ppo_hist', 'momentum_pvo', 'momentum_pvo_signal',\n",
      "       'momentum_pvo_hist', 'momentum_kama', 'others_dr', 'others_dlr',\n",
      "       'others_cr'],\n",
      "      dtype='object')\n",
      "DataFrame head:                  open       high        low      close     volume\n",
      "Date                                                             \n",
      "2019-06-20  50.092499  50.152500  49.507500  49.865002   86056000\n",
      "2019-06-21  49.700001  50.212502  49.537498  49.695000  191202400\n",
      "2019-06-24  49.634998  50.040001  49.542500  49.645000   72881600\n",
      "2019-06-25  49.607498  49.814999  48.822498  48.892502   84281200\n",
      "2019-06-26  49.442501  50.247501  49.337502  49.950001  104270000\n"
     ]
    },
    {
     "ename": "FutureWarning",
     "evalue": "Series.__setitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To set a value by position, use `ser.iloc[pos] = value`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/tradeEnv/lib/python3.10/site-packages/pandas/core/series.py:1298\u001b[0m, in \u001b[0;36mSeries.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   1297\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1298\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_with_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m   1300\u001b[0m     \u001b[38;5;66;03m# We have a scalar (or for MultiIndex or object-dtype, scalar-like)\u001b[39;00m\n\u001b[1;32m   1301\u001b[0m     \u001b[38;5;66;03m#  key that is not present in self.index.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tradeEnv/lib/python3.10/site-packages/pandas/core/series.py:1370\u001b[0m, in \u001b[0;36mSeries._set_with_engine\u001b[0;34m(self, key, value, warn)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_with_engine\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value, warn: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1370\u001b[0m     loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1372\u001b[0m     \u001b[38;5;66;03m# this is equivalent to self._values[key] = value\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tradeEnv/lib/python3.10/site-packages/pandas/core/indexes/datetimes.py:627\u001b[0m, in \u001b[0;36mDatetimeIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# unrecognized type\u001b[39;00m\n\u001b[0;32m--> 627\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 3",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFutureWarning\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m num_envs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m      2\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAAPL_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m envs \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSyncVectorEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmake_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_days_to_consider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_envs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Initialize Agent and Train\u001b[39;00m\n\u001b[1;32m      6\u001b[0m agent \u001b[38;5;241m=\u001b[39m DoubleQLearningAgent(envs, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, discount_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tradeEnv/lib/python3.10/site-packages/gymnasium/vector/sync_vector_env.py:53\u001b[0m, in \u001b[0;36mSyncVectorEnv.__init__\u001b[0;34m(self, env_fns, observation_space, action_space, copy)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Vectorized environment that serially runs multiple environments.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m        (or, by default, the observation space of the first sub-environment).\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv_fns \u001b[38;5;241m=\u001b[39m env_fns\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs \u001b[38;5;241m=\u001b[39m [env_fn() \u001b[38;5;28;01mfor\u001b[39;00m env_fn \u001b[38;5;129;01min\u001b[39;00m env_fns]\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmetadata\n",
      "File \u001b[0;32m~/miniconda3/envs/tradeEnv/lib/python3.10/site-packages/gymnasium/vector/sync_vector_env.py:53\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Vectorized environment that serially runs multiple environments.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m        (or, by default, the observation space of the first sub-environment).\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv_fns \u001b[38;5;241m=\u001b[39m env_fns\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs \u001b[38;5;241m=\u001b[39m [\u001b[43menv_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m env_fn \u001b[38;5;129;01min\u001b[39;00m env_fns]\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmetadata\n",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m num_envs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m      2\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAAPL_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m envs \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mvector\u001b[38;5;241m.\u001b[39mSyncVectorEnv([\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43mmake_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_days_to_consider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_envs)])\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Initialize Agent and Train\u001b[39;00m\n\u001b[1;32m      6\u001b[0m agent \u001b[38;5;241m=\u001b[39m DoubleQLearningAgent(envs, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, discount_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m)\n",
      "File \u001b[0;32m~/Projects/github_clones/Q(anon)Trader/src/environment.py:70\u001b[0m, in \u001b[0;36mmake_env\u001b[0;34m(file_path, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path, index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m, parse_dates\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame head: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mhead()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Debugging line\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43madd_all_ta_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mopen\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhigh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhigh\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlow\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclose\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvolume\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvolume\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m df\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame with TA features head: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mhead()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Debugging line\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tradeEnv/lib/python3.10/site-packages/ta/wrapper.py:589\u001b[0m, in \u001b[0;36madd_all_ta_features\u001b[0;34m(df, open, high, low, close, volume, fillna, colprefix, vectorized)\u001b[0m\n\u001b[1;32m    570\u001b[0m df \u001b[38;5;241m=\u001b[39m add_volume_ta(\n\u001b[1;32m    571\u001b[0m     df\u001b[38;5;241m=\u001b[39mdf,\n\u001b[1;32m    572\u001b[0m     high\u001b[38;5;241m=\u001b[39mhigh,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    578\u001b[0m     vectorized\u001b[38;5;241m=\u001b[39mvectorized,\n\u001b[1;32m    579\u001b[0m )\n\u001b[1;32m    580\u001b[0m df \u001b[38;5;241m=\u001b[39m add_volatility_ta(\n\u001b[1;32m    581\u001b[0m     df\u001b[38;5;241m=\u001b[39mdf,\n\u001b[1;32m    582\u001b[0m     high\u001b[38;5;241m=\u001b[39mhigh,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    587\u001b[0m     vectorized\u001b[38;5;241m=\u001b[39mvectorized,\n\u001b[1;32m    588\u001b[0m )\n\u001b[0;32m--> 589\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43madd_trend_ta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhigh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhigh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfillna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfillna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolprefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvectorized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvectorized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    598\u001b[0m df \u001b[38;5;241m=\u001b[39m add_momentum_ta(\n\u001b[1;32m    599\u001b[0m     df\u001b[38;5;241m=\u001b[39mdf,\n\u001b[1;32m    600\u001b[0m     high\u001b[38;5;241m=\u001b[39mhigh,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    606\u001b[0m     vectorized\u001b[38;5;241m=\u001b[39mvectorized,\n\u001b[1;32m    607\u001b[0m )\n\u001b[1;32m    608\u001b[0m df \u001b[38;5;241m=\u001b[39m add_others_ta(df\u001b[38;5;241m=\u001b[39mdf, close\u001b[38;5;241m=\u001b[39mclose, fillna\u001b[38;5;241m=\u001b[39mfillna, colprefix\u001b[38;5;241m=\u001b[39mcolprefix)\n",
      "File \u001b[0;32m~/miniconda3/envs/tradeEnv/lib/python3.10/site-packages/ta/wrapper.py:379\u001b[0m, in \u001b[0;36madd_trend_ta\u001b[0;34m(df, high, low, close, fillna, colprefix, vectorized)\u001b[0m\n\u001b[1;32m    376\u001b[0m df[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mtrend_aroon_ind\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m indicator_aroon\u001b[38;5;241m.\u001b[39maroon_indicator()\n\u001b[1;32m    378\u001b[0m \u001b[38;5;66;03m# PSAR Indicator\u001b[39;00m\n\u001b[0;32m--> 379\u001b[0m indicator_psar \u001b[38;5;241m=\u001b[39m \u001b[43mPSARIndicator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhigh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mhigh\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlow\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclose\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.02\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfillna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfillna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;66;03m# df[f'{colprefix}trend_psar'] = indicator.psar()\u001b[39;00m\n\u001b[1;32m    388\u001b[0m df[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mtrend_psar_up\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m indicator_psar\u001b[38;5;241m.\u001b[39mpsar_up()\n",
      "File \u001b[0;32m~/miniconda3/envs/tradeEnv/lib/python3.10/site-packages/ta/trend.py:969\u001b[0m, in \u001b[0;36mPSARIndicator.__init__\u001b[0;34m(self, high, low, close, step, max_step, fillna)\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_step \u001b[38;5;241m=\u001b[39m max_step\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fillna \u001b[38;5;241m=\u001b[39m fillna\n\u001b[0;32m--> 969\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tradeEnv/lib/python3.10/site-packages/ta/trend.py:1030\u001b[0m, in \u001b[0;36mPSARIndicator._run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1028\u001b[0m high2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_high\u001b[38;5;241m.\u001b[39miloc[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m high2 \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_psar\u001b[38;5;241m.\u001b[39miloc[i]:\n\u001b[0;32m-> 1030\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_psar\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m high2\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m high1 \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_psar\u001b[38;5;241m.\u001b[39miloc[i]:\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_psar\u001b[38;5;241m.\u001b[39miloc[i] \u001b[38;5;241m=\u001b[39m high1\n",
      "File \u001b[0;32m~/miniconda3/envs/tradeEnv/lib/python3.10/site-packages/pandas/core/series.py:1310\u001b[0m, in \u001b[0;36mSeries.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   1305\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m   1306\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1307\u001b[0m         \u001b[38;5;66;03m# positional setter\u001b[39;00m\n\u001b[1;32m   1308\u001b[0m         \u001b[38;5;66;03m# can't use _mgr.setitem_inplace yet bc could have *both*\u001b[39;00m\n\u001b[1;32m   1309\u001b[0m         \u001b[38;5;66;03m#  KeyError and then ValueError, xref GH#45070\u001b[39;00m\n\u001b[0;32m-> 1310\u001b[0m         \u001b[43mwarnings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwarn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# GH#50617\u001b[39;49;00m\n\u001b[1;32m   1312\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSeries.__setitem__ treating keys as positions is deprecated. \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m   1313\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIn a future version, integer keys will always be treated \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m   1314\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mas labels (consistent with DataFrame behavior). To set \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m   1315\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ma value by position, use `ser.iloc[pos] = value`\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;167;43;01mFutureWarning\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstacklevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfind_stack_level\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1318\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1319\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_values(key, value)\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;66;03m# GH#12862 adding a new key to the Series\u001b[39;00m\n",
      "\u001b[0;31mFutureWarning\u001b[0m: Series.__setitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To set a value by position, use `ser.iloc[pos] = value`"
     ]
    }
   ],
   "source": [
    "num_envs = 3\n",
    "file_path = 'AAPL_data.csv'\n",
    "envs = gym.vector.SyncVectorEnv([lambda: make_env(file_path, number_of_days_to_consider=20) for _ in range(num_envs)])\n",
    "\n",
    "# Initialize Agent and Train\n",
    "agent = DoubleQLearningAgent(envs, learning_rate=0.001, discount_factor=0.95)\n",
    "\n",
    "agent, reward_across_episodes, epsilons_across_episodes = q_learning_learning_loop(\n",
    "    envs,\n",
    "    agent,\n",
    "    learning_rate=0.001,\n",
    "    discount_factor=0.95,\n",
    "    episodes=40000,\n",
    "    min_epsilon_allowed=0.01,\n",
    "    initial_epsilon_value=1,\n",
    "    buffer_size=10000,\n",
    "    batch_size=128,\n",
    "    decay_method=\"exponential\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed4414d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_learned_policy(env, agent):\n",
    "    obs = env.reset()\n",
    "    terminated, truncated = np.array([False] * env.num_envs), np.array([False] * env.num_envs)\n",
    "    total_rewards = np.zeros(env.num_envs)\n",
    "\n",
    "    while not np.all(terminated) and not np.all(truncated):\n",
    "        actions = [np.argmax(agent.q_table1.get(tuple(observation.flatten()), np.zeros(env.action_space.n)) + agent.q_table2.get(tuple(observation.flatten()), np.zeros(env.action_space.n))) for observation in obs]\n",
    "        obs, rewards, terminated, truncated, _ = env.step(actions)\n",
    "        total_rewards += rewards\n",
    "\n",
    "    logging.info(\"Total Rewards: %s\", total_rewards)\n",
    "    return total_rewards\n",
    "\n",
    "total_rewards = run_learned_policy(envs, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5821cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grid(env, agent, reward_across_episodes: list, epsilons_across_episodes: list) -> None:\n",
    "    env.train = False\n",
    "    total_reward_learned_policy = [run_learned_policy(env, agent) for _ in range(30)]\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(reward_across_episodes, 'ro')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward Value')\n",
    "    plt.title('Rewards Per Episode (Training)')\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(total_reward_learned_policy, 'ro')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward Value')\n",
    "    plt.title('Rewards Per Episode (Learned Policy Evaluation)')\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(reward_across_episodes)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Cumulative Reward Per Episode (Training)')\n",
    "    plt.title('Cumulative Reward vs Episode')\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(epsilons_across_episodes)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Epsilon Values')\n",
    "    plt.title('Epsilon Decay')\n",
    "    plt.grid()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_grid(envs, agent, reward_across_episodes, epsilons_across_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25de7bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_trading_environment = StockTradingEnvironment('./AAPL_data.csv', number_of_days_to_consider=30)\n",
    "stock_trading_environment.train = False\n",
    "obs, _ = stock_trading_environment.reset()\n",
    "terminated, truncated = False, False\n",
    "while not terminated:\n",
    "    # Convert observation to a tuple\n",
    "    obs_tuple = tuple(obs.flatten())\n",
    "    action = np.argmax(agent.q_table1.get(obs_tuple, np.zeros(envs.action_space.n)) + agent.q_table2.get(obs_tuple, np.zeros(envs.action_space.n)))\n",
    "    obs, reward, terminated, truncated, info = stock_trading_environment.step(action)\n",
    "\n",
    "stock_trading_environment.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba8d1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle(agent, 'aapl_q_learning_agent.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b416066",
   "metadata": {},
   "source": [
    "#### Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2ec473",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = load_pickle(\"aapl_q_learning_agent.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e23d8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_trading_environment = make_env('AAPL_data.csv', number_of_days_to_consider=30)\n",
    "stock_trading_environment.train = False\n",
    "obs, _ = stock_trading_environment.reset()\n",
    "terminated, truncated = False, False\n",
    "while not terminated:\n",
    "    obs_tuple = tuple(obs.flatten())\n",
    "    action = np.argmax(agent.q_table1.get(obs_tuple, np.zeros(envs.action_space.n)) + agent.q_table2.get(obs_tuple, np.zeros(envs.action_space.n)))\n",
    "    obs, reward, terminated, truncated, info = stock_trading_environment.step(action)\n",
    "\n",
    "stock_trading_environment.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
